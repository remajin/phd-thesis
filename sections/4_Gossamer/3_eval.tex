\section{Evaluation}
\label{sg:eval}

In this section we use microbenchmarks to evaluate \name{} on throughput and latency. We also use two application
benchmarks to evaluate how \name{} performs in the real world. These experiments were performed on the r6615 
cluster from Cloudlab Clemson using 12 machines. Each machine here uses AMD EPYC 9354P CPU and has 32 threads 
over four NUMA nodes at 3.3 GHz. We do not use hyperthreading for this experiment. Each machine has 100 Gbps 
Mellanox ConnectX-6 interfaces and connected to a Dell Z9432 switch. These machines are equipped with 188 GB RAM 
each and are running Ubuntu 22.04.2 LTS (GNU/Linux 5.15.0-160-generic x86\_64).


\subsection{Microbenchmark: Echo Throughput}
\input{figures/echo_tput_r6615}

We use an echo application to evaluate the throughput scaling of \name{} as the number of machines in the rack 
increases. For this experiment, each trustee/server sends the same data it received as a response. Each thread 
acts as both client and server. Clients choose a server at random form a uniform distribution and send a request 
containing 32 bytes of data. Each client sends ten million asynchronous echo requests. We compare against eRPC 
\cite{225980} and an MPI library used by and developed for graph500 reference benchmark \cite{g500}.

\ref{fig:echo_tput} shows the results of our experiment. Here we use 16 threads per machine as the eRPC benchmark
only supports up to two NUMA nodes. Each successive data point has one more machine than the previous one, resulting 
in an increment of 16 threads. eRPC starts with 2.9 MOPs per thread and maintains that as more threads are
added. This is in line with their reported performance in a github issue \cite{erpc_issue} for two machines 
connected with a 100 Gbps connection. \name{} starts with double the throughput and scales 3x better than eRPC.
Similarly, though the MPI application starts out performing better than \name{}, it falls behind quickly as the 
number of machines increases. 
% This is because MPI is not well suited to continously sending asynchronous requests 
% back and forth with no other work being done in between.

\input{figures/echo_tput_reqsize_r6615}

\ref{fig:echo_reqsize} shows the performance of all three systems across a range of request sizes. The MPI system 
performs better than both \name{} and eRPC due to smaller request headers, however as the request size 
increases, this advantage becomes negligible and the other systems overtake it. All three systems experience a 
drop in the number of requests served as the request size increases, however \name{} experiences a particularly 
steep drop as it cross the 1KB threshold. This is explained by the TCP fallback for large requests as discussed 
in Section \ref{s:tcp_fallback}. 

\subsection{Microbenchmark: Echo Latency}
\input{figures/erpc_vs_gsm_loaded_latency}

Next we evaluate 99.9th percentile latency under load for eRPC and \name{}. We exclude the MPI application from 
this experiment as the MPI library we use is highly optimized for throughput at the cost of latency, resulting in 
it having minimum latency of 1 millisecond. This experiment also uses the echo application with a request size of 32 bytes. As both 
\name{} and eRPC use batching for asynchronous requests, load here is represented by the number of outstanding 
requests at any given time.

\ref{fig:erpc_gsm1} shows the results of this experiment. Each data point here represents a doubling of batch 
size until 32 and then an increment of 16. Both system have increased throughput and latency as the number of 
outstanding requests increases, however eRPC does better at smaller loads in terms of both latency and throughput. 
This is because of the optimizations discussed in Section \ref{sg:slots}, that result in better performance at higher 
load at the cost of small performance penalty at lower loads, along with the overhead of switching fibers being 
significant at lower load. eRPC then quickly reaches its peak throughput and any increase in load only increases 
the latency whereas \name{} is able to achieve much higher throughput while remaining below 100 \textmu s tail 
latency.

\subsection{Application: Key-Value Store}
\input{figures/kv_r6615_ycsbc_u}
\input{figures/kv_r6615_ycsbb_u}
\input{figures/kv_r6615_ycsbc_z}
\input{figures/kv_r6615_ycsbb_z}

To understand the real world performance of \name{}, we implemented a distributed key-value store named Hydra. 
Hydra divides the key space among all of the server machines equally and each server in turn divides the relevant key 
space among a set of dedicated trustees. A client can connect with any server that is part of the store and submit
queries. Hydra uses \name{} to respond the queries using delegation. We compare Hydra with Redis Cluster \cite {redis}, a common 
production ready key-value store. We also compare against a few single machine stores running independently on the servers, in 
which case, the client needs to divide the key space (also called sharding) and send queries to the appropriate 
server. These experiments are denoted ``client sharded" in the figures. The single machine key-value stores include a delegation based hashmap called trust here, Dashmap from Rust 
crate ``dashmap" version ``6.1.0" \cite{dashmap6} and a hashmap with 4096 shards protected by a readers-writers lock referred as 
Rwlock below. In case of Redis, we disable replication and disk backup and provide two I/O threads per instance. 
Each server machine runs eight Redis instances. This configuration was selected after extensive experimentation 
to ensure maximum performance. The key space 
these experiments consists of one million keys and queries consist of 8 byte keys and 16 byte values. Each reported 
data point here is an average of three runs after an initial warm up stage that populates all the keys. For these 
experiments up to 8 machines act as servers and up to 4 machines act as clients with each client machine running 
256 threads over 64 cores. For Hydra, each client thread connects to one server using TCP. However, for trust, 
Dashmap and Rwlock, each thread needs to connect with each server.

\ref{fig:kv_ycsbc_u} and \ref{fig:kv_ycsbb_u} show the number of operations per second for a read only workload and 
5\% write, 95\% read workload respectively with a uniform distribution of keys. Hydra scales much better than 
running independent servers and leaving the sharding to the clients. This is explained by the fact that for Hydra, 
a client can send a whole batch of requests to any of the servers regardless of where the keys may reside, but for 
single machine stores, the client needs to divide the requests based on the keys and send them to appropriate 
servers. This reduces the batch size and increases the number of packets being sent over the network. That in turn 
results in the servers spending more time processing the TCP communication. This is evidenced by perf, the Linux 
performance analysis tool \cite{linuxperf}, showing that 
a trust server, which is also delegation based, spends 30\% more time in kernel mode than a Hydra server. Dashmap 
and Rwlock do better than trust due to low contention but suffer from higher system call overhead much the same. 
Redis is a production ready key-value store that typically offers 400,000 to 600,000 operations per second 
depending on workload \cite{redis-bench}. This is significantly less than per thread performance of all of the 
stores in this experiment. We also include a line labeled ``redis hypothetical'', showing how it would scale were 
it to have the same per thread performance as Hydra.

\ref{fig:kv_ycsbc_z} and \ref{fig:kv_ycsbb_z} show the number of operations per second for a read only workload and 
5\% write, 95\% read workload respectively with a Zipfian distribution of keys. We see similar trends for read only 
workload but Dashmap and Rwlock scale particularly poorly with the mixed workload due to high contention resulting 
from most of the queries being about a small number of keys. The contention stems from Rwlock and Dashmap, which 
also uses an Rwlock internally, not allowing any readers while the value for a particular key is being updated. 
This is not an issue with either of the delegation based systems as reads and writes are treated the same way.

\subsection{Application: Single Source Shortest Path (SSSP)}
\input{figures/sssp_threads_n27_r6615}
\input{figures/sssp_size_r6615}

For this benchmark, we implement a distributed version of Dijkstra's algorithm 
for shortest path to each node in a weighted undirected graph given a source node. We compare this application to 
the graph500 reference benchmark \cite{g500} for supercomputers. Even though the graph500 benchmark uses the 
delta-stepping algorithm, we observed that both systems traverse roughly equal number of edges, making any 
algorithmic differences negligible. The graphs for these experiments were generated by graph500's implementation 
of kronecker generator with an edge factor of 32. For each experiment we compute Single Source Shortest Path (SSSP) 
on eight randomly chosen source nodes and report the average time taken per run. SSSP involves computing the 
shortest path to all reachable nodes in the graph, given a source node.

\ref{fig:sssp_threads_n27} shows the results of SSSP on a graph with $2^{27}$ nodes and average degree of 32, 
as the size of the cluster increases. Each successive data point increments the thread count by 32. Even though
graph500 is a highly specialized application designed to benchmark supercomputers, \name{} is still able to match 
its performance while remaining a general-purpose framework. Similarly, \ref{fig:sssp_size} shows \name{} 
achieving comparable performance to graph500 across a range of graph sizes with 384 threads across 12 machines.





% \input{figures/erpc_vs_gsm_loaded_latency_t56}
% \input{figures/sssp_threads_n26_r6615}
% \input{figures/qp_scaling_5m_r6615}
% \input{figures/qp_scaling_6m_r6615}
