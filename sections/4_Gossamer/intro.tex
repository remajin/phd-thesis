\section{Introduction}

Computing needs have grown steadily, pushing systems from simple, single-threaded execution toward far more 
parallel and scalable designs. Eventually, even multicore machines proved insufficient for the larger workloads, 
inspiring a shift toward distributed computing systems composed of many networked machines working together to 
handle massive data volumes, deliver high availability, and scale horizontally.

This chapter extends \trust{}, 
presented in the previous chapter, to build a rack-wide programming framework. \trust{} is 
based on delegation where a few threads (servers) act on behalf of many threads (clients). Its aim is to provide 
single threaded data structure performance in a multi-threaded setting. \name{} takes it one step further and aims 
to do the same in multi-machine setting. \name{} provides a very similar API to \trust{} enabling programmers to
design applications like a normal delegation application with very few changes. These applications give the 
illusion of running on a single machine even though they may be distributed across many nodes. Most of the 
distribution of work is handled behind the scenes, but programmers have the option to customize where the worker 
threads should run and where data should be held in the rack if they so choose.

\name{} is a rack-wide computing framework. We define rack as a set of machines connected by a single switch, 
also called a Top-of-Rack (ToR) switch. Our goal is to have the abstraction of a single application running across 
the rack. To this end we treat the rack as a single machine and if a link or a node goes down the whole rack is 
considered inoperable. Similarly, if a part of the application, running on any of the nodes crashes, the whole 
application terminates. These aspects limit the size of the rack to a small number of machines, where we expect 
node failures to be very rare.

A key challenge that limits performance in a rack-wide system is the additional latency introduced by network 
communication. \name{} mitigates this by combining RDMA with as fibers. RDMA offers sub-microsecond one-way 
latency, significantly faster than accessing persistent storage on a single machine, though still slower than 
local memory and considerably slower than CPU cache accesses. Fibers help bridge this latency gap by allowing 
applications to issue and manage a large 
number of concurrent, outstanding requests. This is further amplified by batching the requests generated by all 
the fibers on a single system thread. While RDMA reduces latency, it does not scale efficiently as the number of 
connections per machine grows. To address this, \name{} maintains only a single RDMA connection per hardware 
thread for each remote machine.