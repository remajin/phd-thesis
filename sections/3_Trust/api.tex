\section{Core API}
\label{s:nesting}
The \trust{} API supports a variety of ways to delegate work, some of which we elide due to space constraints. Below, we describe the core functions in detail.

\subsection{\texttt{apply()}: synchronous delegation}
\begin{lstlisting}[numbers=none]
apply(c: FnOnce(&mut T)->U)->U
\end{lstlisting}

{\tt apply()} is the primary function for blocking, synchronous delegation as described in earlier sections. 
It takes a closure of the form {\tt |\&mut T| \{\}}, where {\tt T} is the type of the property. 
If the closure has a return value, apply returns this value to the caller. 

Importantly, \texttt{apply()} is synchronous, suspending the current fiber until the operation has completed.
Often, the best performance with {\tt apply()} is achieved when running multiple application fibers per thread. 
Then, while one fiber is waiting for its response, another may productively use the CPU.

\pagebreak

\subsection{\texttt{apply\_then()}: non-blocking delegation}
\begin{lstlisting}[numbers=none]
  apply_then(c: FnOnce(&mut T)->U, 
          then: FnOnce(U))
\end{lstlisting}
  

\begin{figure*}[ht]
  \centering
    \begin{lstlisting}[
		basicstyle=\small, %or \small or \footnotesize etc.
	]
let ct = trustee.entrust(17);           // create trust for shared counter set to 17
ct.apply_then(|c| { *c+=1; *c },         // increment counter and return its value
              |val| assert!(val==18));  // check return value once received
\end{lstlisting}
\caption{Asynchronous version of the example in \ref{f:minimal}.
The second closure runs on the client, once the result of the first closure is received from the trustee. }
\label{f:minimal_async}
\end{figure*}

Frequently, asynchronous (or non-blocking) application logic can allow the programmer to express additional concurrency either without running multiple fibers, or in combination with multiple fibers. 
Here, {\tt apply\_then()} returns to the caller without blocking, and does not produce a return value.
Instead, the second closure, {\tt then}, is called with the return value from the delegated closure, once it has been received. 
\ref{f:minimal_async} demonstrates the use of {\tt apply\_then()} following the pattern of \ref{f:minimal}.

The {\tt then}-closure is a very powerful abstraction, as it too is able to capture variables from the local environment, 
allowing it to perform tasks like adding the return value (once available) to a vector accessible to the caller. 
Here, Rust's strict lifetime rules automatically catch otherwise easily introduced use-after-free and dangling pointer problems, forcing
the programmer to appropriately manage object lifetime either through scoping or reference counted heap storage. 

%Similar to {\tt apply()}, the request is appended to the local pending request queue, and transmitted once a request line becomes available. 
Importantly, as {\tt apply\_then()} does not suspend the caller, it may freely be called from within delegated context. 

\subsection{\texttt{launch()}: apply in a trustee-side fiber}
\begin{lstlisting}[numbers=none]
  launch(c: FnOnce(&mut T)->U)->U
  launch_then(c: FnOnce(&mut T)->U, 
           then: FnOnce(U))

\end{lstlisting}

The most significant constraint imposed by \trust{} on the closure passed to {\tt apply()} and {\tt apply\_then()} is that the closure itself may not block. 
Blocking in delegated context means putting the trustee itself to sleep, preventing it from serving other requests, potentially resulting in deadlock.
In previous work \cite{rcl}, this problem was addressed by maintaining multiple server OS threads, and automatically switching to the next server when one server thread blocks. This avoids blocking the trustee, but imposes high overhead, resulting in considerably lower performance, as demonstrated in \cite{ffwd}.

In \trust{}, blocking in delegated context is prohibited: attempted suspensions in delegated context are detected at runtime, resulting in an assertion failure. Closures may still use {\tt apply\_then()}, but not the blocking {\tt apply()}. 
\footnote{Other forms of blocking, such as I/O waits or scheduler preemption, do not result in assertion failures. However, these can significantly impact performance if common, as blocking the trustee can prevent other threads from making progress.}

The lack of {\it nested blocking delegation} can be a significant constraint on the developer, and perhaps the most important limitation of \trust{}. 
Specifically, it affects modularity, as a library function that blocks internally, even on delegation calls, cannot be used from within delegated context.  

\begin{figure*}
  \centering\
  \includegraphics[height=1.8in]{figures/launch}
  \caption{Operation of {\tt launch()} vs {\tt apply()}. {\tt launch()} supports blocking calls, including nested delegation calls in the
  delegated closure, but incurs a higher minimum overhead. Solid arrows indicate requests, dotted arrows are delegation responses.}
  \label{f:launch}
\end{figure*}

To address this, without sacrificing the performance of the more common case, we provide a convenience function: {\tt launch()}, which offers all the same functionality as {\tt apply()}, but without the blocking restriction.
\ref{f:launch} describes {\tt launch()} from an implementation standpoint. {\tt launch()} creates a temporary fiber on the trustee's thread, which runs the closure. If this fiber is suspended, the client is notified, and the trustee continues to serve the next request. 
Once the temporary fiber resumes and completes execution of the closure, it then delivers the return value and resumes the client fiber via a second delegation call. Thus, if a delegated closure fails the runtime check for blocking calls, the developer can fix this by replacing the {\tt apply()} call, with a {\tt launch()} call. 

% The Rust type system does not let us statically detect whether a closure needs to be delegated with {\tt launch()}. Detecting it at runtime is trivial. 
% Automatically selecting {\tt apply()} or {\tt launch()} at runtime is feasible, but not without imposing significant overhead on the fast path.
% In general, because {\tt launch()} incurs higher overhead than {\tt apply()} (see \S\ref{s:eval_launch} for details), it is better used only as needed.
% That said, a developer who is eager to avoid any chance of a runtime assertion failure can simply substitute every {\tt apply()} call in their code with a {\tt launch()} call.

\subsubsection{Atomicity and \texttt{launch()}}
That said, a complicating factor with blocking closures executed by {\tt launch()} is that without further protection, property accesses are no longer guaranteed to be atomic:
while the newly created fiber is suspended, another delegation request may be applied to the property, resulting in a race condition. 
To avoid this risk, {\tt launch()} is implemented only for {\tt Trust<Latch<T>{>}}.
{\tt Latch<T>} is a wrapper type which provides mutual exclusion, analogous to {\tt Mutex<T>} except that it uses no atomic instructions, and thus may only be accessed by the fibers of a single thread.\footnote{In Rust terms, {\tt Latch<T>} does not implement {\tt Sync}.}
%More on nested blocking delegation requests in \S\ref{s:nested}.

\subsubsection{Leveraging Rust for safe and efficient delegation}
\label{s:noref}

Using the Rust type system, we ensure that delegated closures in \trust{} cannot capture values that contain any references or pointers. 

In principle, this is far stricter than what is necessary: the existing and pervasive Rust traits {\tt Send} and {\tt Sync} already describe
the types that may be safely moved and shared between threads, and this continues to hold within \trust{}. 

That said, safety does not guarantee performance. A common performance pitfall when writing delegation-based software is 
memory stalls on the trustee, which affects trustees disproportionately due to the polling nature of the delegation channel (see Section \ref{s:delegating}). Frequent cache misses and use of atomic instructions in delegated closures can substantially degrade trustee throughput vs. running closures with good memory locality. 

Generally speaking, cache line contention and use of atomic instructions are a natural result of sharing memory between threads. By prohibiting the capture of references and pointers, \trust{} makes accidental shared memory patterns of programming much less likely in delegated code, and encourage cache friendly pass-by-value practices. 

\subsubsection{Variable-size and other heap-allocated values}

Rust closures very efficiently and conveniently capture their environment, which {\tt apply()} sends whole-sale to the trustee. 
However, only types with a size known at compile time may be captured in a Rust closure (or even allocated on the stack). 

In conventional Rust code, variable size types, including strings, are stored on the heap, and referenced by a {\tt Box$<$T$>$} smart pointer. 
For the reasons described above (see Section \ref{s:noref}), we do not allow {\tt Box$<$T$>$} or other types that include pointers or references to be captured in a closure:
only pure values may pass through the delegation channel. 

As a result, variable size objects and other heap-allocated objects must be passed as explicit arguments rather than captured, so that they may be serialized before transmission over the delegation channel. 
For example, a {\tt Box$<$[u8]$>$} (a reference to a heap-allocated variable-sized array of bytes) cannot traverse the delegation channel. Instead, we encode a copy of the variable number of bytes in question into the channel, and pass this value to the closure when it is executed by the trustee. 
In practice, this takes the form of a slightly different function signature.

\begin{lstlisting}[numbers=none]
apply_with(c: FnOnce(&mut T, V)->U, w: V)->U
\end{lstlisting}

Here, the {\tt w:} argument is any type {\tt V:Serialize+Deserialize}, using the popular traits from the {\tt serde} crate. That is, any type that can be serialized and deserialized, may pass over the delegation channel in serialized form.
If more than one argument is needed, these may be passed as a tuple. Thus, to insert a variable-size key and value into an entrusted table, we might use:

\begin{lstlisting}[numbers=none]
  table_trust.apply_with(|table, (key, value)| 
    table.insert(key,value),(key,value))
\end{lstlisting}

We use the efficient {\tt bincode} crate internally for serialization.
As a result, while passing heap-allocated values does incur some additional syntax, the impact in terms of performance is minimal.